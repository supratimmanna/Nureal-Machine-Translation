{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_machine_Translation_3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/MjKy+6IEYc8SQFsjtWnF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QIav0U-uGvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Input, Dense,Embedding\n",
        "from keras.models import Model,load_model\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import model_from_json\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from numpy import array"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gs5tyQKUbeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('mar.txt','r', encoding=\"utf-8\") as f:\n",
        "  data = f.readlines()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RwaMwY5UcsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "13d37558-7e63-4571-e609-127d09305d41"
      },
      "source": [
        "data[:3]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.\\tजा.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #3138228 (sabretou)\\n',\n",
              " 'Run!\\tपळ!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #3138217 (sabretou)\\n',\n",
              " 'Run!\\tधाव!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #3138218 (sabretou)\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqT0Tcopxhek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1b9fed9e-5830-4aff-a4b3-e25ac1188a44"
      },
      "source": [
        "with open('mar.txt','r', encoding=\"utf-8\") as f:\n",
        "  data = f.readlines()\n",
        "#uncleaned_data_list = data.split('\\n')\n",
        "uncleaned_data_list = data[:38695]\n",
        "english_word = []\n",
        "marathi_word = []\n",
        "# cleaned_data_list = []\n",
        "for word in uncleaned_data_list:\n",
        "  english_word.append(word.split('\\t')[:-1][0])\n",
        "  marathi_word.append(word.split('\\t')[:-1][1])\n",
        "  \n",
        "print(marathi_word[:3])\n",
        "\n",
        "language_data = pd.DataFrame(columns=['English','Marathi'])\n",
        "language_data['English'] = english_word\n",
        "language_data['Marathi'] = marathi_word\n",
        "\n",
        "# saving to csv\n",
        "language_data.to_csv('language_data.csv', index=False)\n",
        "\n",
        "# loading data from csv\n",
        "language_data = pd.read_csv('language_data.csv')\n",
        "\n",
        "# remove puntuation\n",
        "def remove_punc(text_list):\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  removed_punc_text = []\n",
        "  for sent in text_list:\n",
        "    sentance = [w.translate(table) for w in sent.split(' ')]\n",
        "    removed_punc_text.append(' '.join(sentance))\n",
        "  return removed_punc_text\n",
        "\n",
        "english_text = language_data['English'].values\n",
        "marathi_text = language_data['Marathi'].values\n",
        "english_text_ = [x.lower() for x in english_text]\n",
        "marathi_text_ = [x.lower() for x in marathi_text]\n",
        "\n",
        "# Text preprocessing\n",
        "english_text_ = [re.sub(\"'\",'',x) for x in english_text_]\n",
        "marathi_text_ = [re.sub(\"'\",'',x) for x in marathi_text_]\n",
        "\n",
        "english_text_ = remove_punc(english_text_)\n",
        "marathi_text_ = remove_punc(marathi_text_)\n",
        "\n",
        "# removing the digits from english sentances\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "removed_digits_text = []\n",
        "for sent in english_text_:\n",
        "  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
        "  removed_digits_text.append(' '.join(sentance))\n",
        "english_text_ = removed_digits_text\n",
        "\n",
        "# removing the digits from the marathi sentances\n",
        "marathi_text_ = [re.sub(\"[२३०८१५७९४६]\",\"\",x) for x in marathi_text_]\n",
        "marathi_text_ = [re.sub(\"[\\u200d]\",\"\",x) for x in marathi_text_]\n",
        "\n",
        "# removing the stating and ending whitespaces\n",
        "english_text_ = [x.strip() for x in english_text_]\n",
        "marathi_text_ = [x.strip() for x in marathi_text_]\n",
        "\n",
        "# Putting the start and end words in the marathi sentances\n",
        "marathi_text_ = [\"start \" + x + \" end\" for x in marathi_text_]\n",
        "\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(lines)\n",
        "      return tokenizer\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "          # integer encode sequences\n",
        "          seq = tokenizer.texts_to_sequences(lines)\n",
        "          # pad sequences with 0 values\n",
        "          seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "          return seq\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "  ylist = list()\n",
        "  for sequence in sequences:\n",
        "    encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "    ylist.append(encoded)\n",
        "  y = array(ylist)\n",
        "  y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "  return y\n",
        "\n",
        "eng_tokenizer=tokenization(english_text_)\n",
        "mar_tokenizer=tokenization(marathi_text_)\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "max_eng_length = max(len(line.split()) for line in english_text_)\n",
        "mar_vocab_size = len(mar_tokenizer.word_index) + 1\n",
        "max_mar_length = max(len(line.split()) for line in marathi_text_)\n",
        "max_mar_length,mar_vocab_size,max_eng_length,eng_vocab_size\n",
        "\n",
        "reverse_word_map_target = dict(map(reversed, mar_tokenizer.word_index.items()))\n",
        "\n",
        "X = english_text_\n",
        "Y = marathi_text_\n",
        "#Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.1)\n",
        "\n",
        "print(len(X_train))\n",
        "\n",
        "print(len(y_train))\n",
        "\n",
        "print(len(X_test))\n",
        "\n",
        "print(len(y_test))\n",
        "\n",
        "trainX = encode_sequences(eng_tokenizer, max_eng_length, X_train)\n",
        "trainY = encode_sequences(mar_tokenizer, max_mar_length,y_train)\n",
        "#target_output=encode_output(trainY,mar_vocab_size)\n",
        "\n",
        "testX = encode_sequences(eng_tokenizer, max_eng_length, X_test)\n",
        "testY = encode_sequences(eng_tokenizer, max_eng_length, y_test)\n",
        "#target_output=encode_output(testY,mar_vocab_size)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['जा.', 'पळ!', 'धाव!']\n",
            "34825\n",
            "34825\n",
            "3870\n",
            "3870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36xudhZ1Vkh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "864cc1b2-08c1-4343-b5d7-8ed4da1a7191"
      },
      "source": [
        "trainX.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34825, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxHGmCLdVBpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_batch(X= X_train,Y=y_train, batch_size=128):\n",
        "  while True:\n",
        "    for j in range(0, len(X), batch_size):\n",
        "      encoder_data_input = np.zeros((batch_size,max_eng_length),dtype='float32') #metrix of batch_size*max_length_english\n",
        "      decoder_data_input = np.zeros((batch_size,max_mar_length),dtype='float32') #metrix of batch_size*max_length_marathi\n",
        "      decoder_target_input = np.zeros((batch_size,max_mar_length,mar_vocab_size),dtype='float32') # 3d array one hot encoder decoder target data\n",
        "      for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n",
        "        for t, word in enumerate(input_text.split()):\n",
        "          encoder_data_input[i,t] = eng_tokenizer.word_index[word] # Here we are storing the encoder \n",
        "                                                                      #seq in row here padding is done automaticaly as \n",
        "                                                                      #we have defined col as max_lenght\n",
        "        for t, word in enumerate(target_text.split()):\n",
        "          # if word == 'START_':\n",
        "          #   word = 'start'\n",
        "          # elif word == 'END_':\n",
        "          #   word = 'end'\n",
        "          decoder_data_input[i,t] = mar_tokenizer.word_index[word] # same for the decoder sequence\n",
        "          if t>0:\n",
        "            decoder_target_input[i,t-1,mar_tokenizer.word_index[word]] = 1 #target is one timestep ahead of decoder input because it does not have 'start tag'\n",
        "      # print(encoder_data_input.shape())\n",
        "      yield ([encoder_data_input,decoder_data_input],decoder_target_input)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t89v3yKpVHT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 50\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None,),name=\"encoder_inputs\")\n",
        "emb_layer_encoder = Embedding(eng_vocab_size,latent_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(emb_layer_encoder)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,),name=\"decoder_inputs\")\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "emb_layer_decoder = Embedding(mar_vocab_size,latent_dim, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\n",
        "decoder_dense = Dense(mar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjoHzJlgVLOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEmZ85oLYwjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model_2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model_weight_5.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# loading the model architecture and asigning the weights\n",
        "json_file = open('model_2.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model_loaded = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "model_loaded.load_weights(\"model_weight_5.h5\")\n",
        "\n",
        "latent_dim = 50\n",
        "#inference encoder\n",
        "encoder_inputs_inf = model_loaded.input[0] #Trained encoder input layer\n",
        "encoder_outputs_inf, inf_state_h, inf_state_c = model_loaded.layers[4].output # retoring the encoder lstm output and states\n",
        "encoder_inf_states = [inf_state_h,inf_state_c]\n",
        "encoder_model = Model(encoder_inputs_inf,encoder_inf_states)\n",
        "\n",
        "\n",
        "#inference decoder\n",
        "# The following tensor will store the state of the previous timestep in the \"starting the encoder final time step\"\n",
        "decoder_state_h_input = Input(shape=(latent_dim,)) #becase during training we have set the lstm unit to be of 50\n",
        "decoder_state_c_input = Input(shape=(latent_dim,))\n",
        "decoder_state_input = [decoder_state_h_input,decoder_state_c_input]\n",
        "\n",
        "# # inference decoder input\n",
        "decoder_input_inf = model_loaded.input[1] #Trained decoder input layer\n",
        "# decoder_input_inf._name='decoder_input'\n",
        "decoder_emb_inf = model_loaded.layers[3](decoder_input_inf)\n",
        "decoder_lstm_inf = model_loaded.layers[5]\n",
        "decoder_output_inf, decoder_state_h_inf, decoder_state_c_inf = decoder_lstm_inf(decoder_emb_inf, initial_state =decoder_state_input)\n",
        "decoder_state_inf = [decoder_state_h_inf,decoder_state_c_inf]\n",
        "#inference dense layer\n",
        "dense_inf = model_loaded.layers[6]\n",
        "decoder_output_final = dense_inf(decoder_output_inf)# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "decoder_model = Model([decoder_input_inf]+decoder_state_input,[decoder_output_final]+decoder_state_inf)\n",
        "\n",
        "# Code to predct the input sentences translation\n",
        "def decode_seq(input_seq):\n",
        "  # print(\"input_seq=>\",input_seq)\n",
        "  state_values_encoder = encoder_model.predict(input_seq)\n",
        "  # intialize the target seq with start tag\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = mar_tokenizer.word_index['start']\n",
        "  # print(\"target_seq:=>\",target_seq)\n",
        "  stop_condition = False\n",
        "  decoder_sentance = ''\n",
        "  # print(\"Beforee the while loop\")\n",
        "  while not stop_condition:\n",
        "    sample_word , decoder_h,decoder_c= decoder_model.predict([target_seq] + state_values_encoder)\n",
        "    # print(\"sample_word: =>\",sample_word)\n",
        "    sample_word_index = np.argmax(sample_word[0,-1,:])\n",
        "    # print(\"sample_word_index: \",sample_word_index)\n",
        "    decoder_word = reverse_word_map_target[sample_word_index]\n",
        "    decoder_sentance += ' '+ decoder_word\n",
        "    # print(\"decoded word:=>\",decoder_word)\n",
        "    # print(len(decoder_sentance))\n",
        "    # print(\"len(decoder_sentance) > 70: \",len(decoder_sentance) > 70)\n",
        "    # print('decoder_word == \"end\"',decoder_word == 'end')\n",
        "    # print(decoder_word == 'end' or len(decoder_sentance) > 70)\n",
        "    # stop condition for the while loop\n",
        "    if (decoder_word == 'end' or \n",
        "        len(decoder_sentance) > 30):\n",
        "        stop_condition = True\n",
        "        # print(\"from if condition\")\n",
        "    # target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sample_word_index\n",
        "    # print(target_seq)\n",
        "    state_values_encoder = [decoder_h,decoder_c]\n",
        "  return decoder_sentance\n",
        "\n",
        "for i in range(5):\n",
        "  sentance = X_test[i]\n",
        "  original_target = y_test[i]\n",
        "  input_seq = eng_tokenizer.texts_to_sequences([sentance])\n",
        "  pad_sequence = pad_sequences(input_seq, maxlen= 30,padding='post')\n",
        "  # print('input_sequence =>',input_seq)\n",
        "  # print(\"pad_seq=>\",pad_sequence)\n",
        "  predicted_target = decode_seq(pad_sequence)\n",
        "  print(\"Test sentance: \",i+1)\n",
        "  print(\"sentance: \",sentance)\n",
        "  print(\"origianl translate:\",original_target[6:-4])\n",
        "  print(\"predicted Translate:\",predicted_target[:-4])\n",
        "  print(\"==\"*50)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}